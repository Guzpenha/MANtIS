<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>MANtIS - a multi-domain information seeking dialogues dataset | MANtIS</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="MANtIS - a multi-domain information seeking dialogues dataset" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="a multi-domain information seeking dialogues dataset" />
<meta property="og:description" content="a multi-domain information seeking dialogues dataset" />
<link rel="canonical" href="http://localhost:4000/" />
<meta property="og:url" content="http://localhost:4000/" />
<meta property="og:site_name" content="MANtIS" />
<script type="application/ld+json">
{"name":"MANtIS","description":"a multi-domain information seeking dialogues dataset","@type":"WebSite","url":"http://localhost:4000/","headline":"MANtIS - a multi-domain information seeking dialogues dataset","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=1b8a9a0900f735a851b1dc340dd812a02b09d9ed">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="http://localhost:4000/">MANtIS</a></h1>
        
<!--         
          <img src="/img/lambda-lab.png" alt="Logo" width="50%" height="50%" />
         -->

        <p>a multi-domain information seeking dialogues dataset</p>


<!--         
        <p class="view"><a href="https://github.com/Guzpenha/mantis">View the Project on GitHub <small>Guzpenha/mantis</small></a></p>
         -->

        <ul class="downloads">
          <li><a href="https://drive.google.com/file/d/1cWEbTC4klLQDLej--IG2OAZIT4AX549A/view" target="_blank">Download<strong>Complete</strong></a></li>
          <li><a href="https://drive.google.com/file/d/1JI9VAuHllyZxr7XhTYLhx7iI2EVd3-a4/view" target="_blank">Download <strong>Intent</strong></a></li>
          <li><a href="https://drive.google.com/file/d/1nf_JRR7zIcCLrzvL_vRsuzBxDcD_3g6N/view" target="_blank">Download<strong>R-ranking</strong></a></li>
        </ul>

        <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/software-technology/web-information-systems/projects/lambda-lab/" target="_blank"> <img src="/img/lambda-lab.png" alt="Logo" width="30%" height="30%" float:left;/></a>

        <a href="https://www.tudelft.nl/en/" target="_blank"><img src="/img/tu-delft.jpg" alt="Logo" width="25%" height="25%" float:left;/> </a>

      </header>
      <section>

      <h1 id="mantis---a-multi-domain-information-seeking-dialogues-dataset">MANtIS - a multi-domain information seeking dialogues dataset</h1>

<h2 id="introduction">Introduction</h2>
<p>MANtIS is a multi-domain dialogue dataset contatining information-seeking interactions from the community  question-answering portal <a href="https://stackexchange.com">Stack Exchange</a>. Unlike previous information-seeking dialogue datasets that focus on only one domain, MANtIS has <strong>diverse</strong> conversations from 14 different sites, such as <em>physics</em>, <em>travel</em> and <em>worldbuilding</em>. Additionaly, all dialogues have a url, providing <strong>grounding</strong> to the conversations. It can be used for the following tasks: conversation response ranking/generation and intent prediction. We provide manually annotated intent labels for more than 1300 multi-turn dialogues. See an example of the annotations on the right side of each utterance of a conversation extracted from the <em>gaming</em> domain:</p>

<p align="center">
<img src="img/MANtIS_DatasetExamples.png" />
</p>

<p>The conversations of the dataset are multi-turn, multi-intent, containing clarification questions and complex information needs, grounded in web pages and extracted from different domains.</p>

<h2 id="mantis---complete-json">MANtIS - complete JSON</h2>
<p>The dataset has over 80,000 dialogues between information seekers and information providers from the following domains of Stack Exchange: apple, askubuntu, dba, diy, electronics, english, gaming, gis, physics, scifi, security, stats, travel and worldbuilding. In order to ensure that each conversation in our dataset follows our set of criteria, we have devised a list of six conditions that must hold for each conversation:</p>

<ol>
  <li>The entire conversation takes place between two users (the information <em>seeker</em> who starts off the conversation and the information <em>provider</em>). Conversations with three or more users are not considered.</li>
  <li>The conversation consists of at least 2 utterances per user.</li>
  <li>At least one of the information provider’s utterances contains a hyperlink (providing grounding).</li>
  <li>The conversation has not been marked as <em>Spam</em> or <em>Offensive</em>.</li>
  <li>The conversation has not been edited or marked as deprecated on the Stack Exchange portal.</li>
  <li>If the final turn in the conversation belongs to the information seeker, it contains <em>positive feedback</em> (identified using the
<a href="https://www.nltk.org/_modules/nltk/sentiment/vader.html">vader score</a>).</li>
</ol>

<p>The processed JSON dataset is available for download <a href="https://drive.google.com/file/d/1cWEbTC4klLQDLej--IG2OAZIT4AX549A/view?usp=sharing">here</a>, with the following format:</p>

<ul>
  <li><strong>dialog_id</strong>: a unique id for a dialog - ids are consecutive</li>
  <li><strong>category</strong>: domain to which the dialogue belongs</li>
  <li><strong>title</strong>: dialog title from the forum</li>
  <li><strong>dialog_time</strong>: the time that the first utterance of the dialog was posted</li>
  <li><strong>utterances</strong>: a list of utterances in this dialog
    <ul>
      <li><strong>actor_type</strong>: <em>user</em> or <em>agent</em> (“user” refers to the information seeker that initiates the conversation. The information provider is considered as “agent”)</li>
      <li><strong>utterance_pos</strong>: the utterance position in the dialog (starts from 1)</li>
      <li><strong>utterance</strong>: the content of the utterance</li>
      <li><strong>votes</strong>: number of votes the answer received from the community</li>
      <li><strong>utterance_time</strong>: the time that the utterance was posted</li>
      <li><strong>is_answer</strong>: whether the utterance is selected as the best answer by the community</li>
      <li><strong>id</strong>: the id of the original post/comment
   (for comments, the syntax is {post_id}_{comment_id})</li>
    </ul>
  </li>
</ul>

<h2 id="mantis---intent-labeled-json">MANtIS - intent labeled JSON</h2>

<p>To further enrich the dataset, we have employed 2 specialist annotators to mark a subset of 1356 dialogues (Krippendorff’s agreement of 0.71) from the dataset with intent labels for each utterance. The following schema was used:</p>

<table>
  <thead>
    <tr>
      <th>Intent</th>
      <th>Description</th>
      <th>Example snippet</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Further Details</td>
      <td>A user (either asking or answering user) provides more details.</td>
      <td>Hi. Sorry for taking so long to reply. The information you need is …</td>
    </tr>
    <tr>
      <td>Follow Up Question</td>
      <td>Asking user asks one or more follow up questions about relevant issues.</td>
      <td>Thanks. I really have one more simple question – if I …</td>
    </tr>
    <tr>
      <td>Information Request</td>
      <td>A user (either asking or answering user) is asking for clarifications or further information.</td>
      <td>What is the make and model of the computer? Have you tried installing … Your advice is not detailed enough. I’m not sure what you mean by …</td>
    </tr>
    <tr>
      <td>Potential Answer</td>
      <td>A potential solution, provided by the answering user.</td>
      <td>Hi. To change the PIN on your phone, you may follow the steps below:..</td>
    </tr>
    <tr>
      <td>Positive Feedback</td>
      <td>Asking user provides positive feedback about the offered solution.</td>
      <td>Hi. That was exactly what I needed. Thanks!</td>
    </tr>
    <tr>
      <td>Negative Feedback</td>
      <td>Asking user provides negative feedback about the offered solution.</td>
      <td>Thanks for you help! However, the fix did not work..</td>
    </tr>
    <tr>
      <td>Greetings / Gratitude</td>
      <td>A user (asking or answering user) offers a greeting or expresses gratitude.</td>
      <td>Thank you for all the responses!</td>
    </tr>
    <tr>
      <td>Other</td>
      <td>Anything that does not fit into the above categories.</td>
      <td>:) :) :) . <em>shrug</em></td>
    </tr>
  </tbody>
</table>

<p>The distribution of labels across all annotated conversations is shown in the figure below, with Original Question, Potential Answer and Further Details being the most frequent labels. 21% of utterances were annotated with more than one label, indicating the multi-intent nature of our dataset.</p>

<p align="center">
<img width="75%" height="75%" src="img/barplot_intents.png" />
</p>

<p>The JSON dataset with the labeled intents is available for download <a href="https://drive.google.com/file/d/1JI9VAuHllyZxr7XhTYLhx7iI2EVd3-a4/view?usp=sharing">here</a></p>

<h2 id="mantis---response-ranking-format">MANtIS - response-ranking format</h2>

<p>We also provide the dataset in a format suited for directly training a neural ranking networks.  Firstly, each conversation is split into chunks of consecutive utterances that have at least 2 utterances per user and where the last utterance is generated by the information <em>provider</em>. Therefore, if a conversation has 3 turns per user, it will have 2 possible contexts (the first 2 and the first 3 utterances per user).</p>

<p>Afterwards, for each such context, we generate negative sampled instances in which the current last utterance of the provider is replaced with a negative sample obtained by using <a href="https://radimrehurek.com/gensim/summarization/bm25.html">BM25</a> with the correct answer as query. For the the mantis_10 variant of the dataset, we randomly generate 10 negative samples from the top 1000 results of BM25, whereas for the mantis_50 variant we generate 50 negative samples. The newly obtained datasets are saved in a <code class="highlighter-rouge">.tsv</code> format in the following format:</p>

<p><code class="highlighter-rouge">label \t utterance_1 \t utterance_2 \t ...... \t candidate_response</code>, where:</p>
<ul>
  <li><code class="highlighter-rouge">label</code> is 1 when the <code class="highlighter-rouge">candidate_response</code> is the true response and 0 when it is a negative sample</li>
  <li><code class="highlighter-rouge">utterance_{x}</code> is a utterance of one of the 2 users</li>
  <li><code class="highlighter-rouge">candidate_response</code> is the answer of the current context</li>
</ul>

<p>The mantis_10 response ranking dataset in <code class="highlighter-rouge">.tsv</code> format is available for download <a href="https://drive.google.com/file/d/1nf_JRR7zIcCLrzvL_vRsuzBxDcD_3g6N/view?usp=sharing">here</a>
The mantis_50 response ranking dataset in <code class="highlighter-rouge">.tsv</code> format is available for download <a href="https://drive.google.com/file/d/11_Um52HzjC41M9S-xSAX6HZP25bmi2oq/view?usp=sharing">here</a></p>

<h2 id="questions-and-citation">Questions and citation</h2>

<p>You can contact us via email to authors (available on the <a href="https://arxiv.org/abs/" target="_blank">paper</a>) or by creating issues in the github project. If you use MANtIS in your work please include a citation to the paper introducing the dataset:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{mantis,
  title={Introducing MANtIS: a novel Multi-Domain Information Seeking Dialogues Dataset},
  author={Gustavo Penha, Alexandru Balan, Claudia Hauff},
  journal={arXiv preprint arXiv:},
  year={2019}
}
</code></pre></div></div>

<h2 id="using-the-code">Using the code</h2>

<p>Considering that Stack Exchange has more than 170 domains and we processed 14 of thoose, we also provide the source code for extracting the dataset for any of the existing domains.</p>

<h4 id="installing-dependencies">Installing dependencies</h4>
<p>In order to install all the required external dependencies, please run <code class="highlighter-rouge">pip install -r requirements.txt</code> in the root folder of the project. 
We recommend using a virtual enviroment with Python &gt;= 3.6.8. Python 2 is not supported.</p>

<p>For NLP, the project uses <a href="https://spacy.io/">spacy</a> and the <a href="https://spacy.io/usage/models">en_core_web_sm</a>
model. To download the model, please run <code class="highlighter-rouge">python -m spacy download en_core_web_sm</code></p>

<h4 id="fetching-the-xml-files-for-each-stackexchange-site">Fetching the .xml files for each stackExchange site</h4>
<p>As mentioned previously, we use StackExchange as it offers a public dump of its conversations. To fetch the initial
<a href="https://archive.org/details/stackexchange">stackExchange dump</a>, you need to run the <code class="highlighter-rouge">fetch_stackexchange_dump.sh</code> script, which
is located in the root folder of the project. This will create a folder called <code class="highlighter-rouge">stackexchange_dump</code> and will put all the <code class="highlighter-rouge">.xml</code>
files there. During the process, it might ask to install a utility to unzip <code class="highlighter-rouge">.7z</code> files. 
However, the downloaded <code class="highlighter-rouge">.xml</code> files are in a structure that makes it difficult to process conversations. For this reason,
we have taken inspiration from <a href="https://ciir.cs.umass.edu/downloads/msdialog">MSDialog’s</a> JSON structure for their dataset and
created a pipeline that aggregates the XML data into a single JSON file.</p>

<h4 id="building-the-json-dataset">Building the JSON dataset</h4>

<p>To run the script that turns the XML dump into a JSON file similar to
<a href="https://ciir.cs.umass.edu/downloads/msdialog/">MSDialog - Complete</a>, you are required to run
(in the root folder) <code class="highlighter-rouge">python run.py json {topic}</code>, where <code class="highlighter-rouge">{topic}</code> is a supported
topic from StackExchange. The updated list of topics is being maintained
<a href="https://github.com/alexanderblnf/conversational-search-dataset/wiki/Supported-Topics">here</a>.
The output is stored in <code class="highlighter-rouge">stackexchange_dump/{topic}/data.json</code>. To obtain a set of datasets 
from a handpicked list of domains, please run use the <code class="highlighter-rouge">run.all.sh</code> script.</p>

<p>In order to merge multiple json datasets into a <strong>single, multi-domain dataset</strong>, you are required
to run <code class="highlighter-rouge">python run.py merge {topic1},{topic2},{topic3}...{topicN}</code>, where <code class="highlighter-rouge">{topicX}</code> is a topic
for which there is already a constructed json dataset. The output is stored in 
<code class="highlighter-rouge">stackexchange_dump/merged_{allocation}.tsv</code>, where <code class="highlighter-rouge">{allocation}</code> is either train, dev or test.</p>

<h5 id="json-data-format">JSON data format:</h5>

<ul>
  <li><strong>dialog_id</strong>: a unique id for a dialog - ids are consecutive</li>
  <li><strong>category</strong>: domain to which the dialogue belongs</li>
  <li><strong>title</strong>: dialog title from the forum</li>
  <li><strong>dialog_time</strong>: the time that the first utterance of the dialog was posted</li>
  <li><strong>utterances</strong>: a list of utterances in this dialog
    <ul>
      <li><strong>actor_type</strong>: <em>user</em> or <em>agent</em> (“user” refers to the information seeker that initiates the conversation. 
  All the other conversation participants are considered as “agents”)</li>
      <li><strong>utterance_pos</strong>: the utterance position in the dialog (starts from 1)</li>
      <li><strong>utterance</strong>: the content of the utterance</li>
      <li><strong>votes</strong>: number of votes the answer received from the community</li>
      <li><strong>utterance_time</strong>: the time that the utterance was posted</li>
      <li><strong>is_answer</strong>: whether the utterance is selected as the best answer by the community</li>
      <li><strong>id</strong>: the id of the original post/comment
   (for comments, the syntax is {post_id}_{comment_id})</li>
    </ul>
  </li>
</ul>

<h4 id="building-the-response-ranking-training-datasets">Building the response ranking training datasets</h4>

<p>To run the script that turns the JSON file to a training dataset similar to 
<a href="https://ciir.cs.umass.edu/downloads/msdialog/">MSDialog - ResponseRank</a>, you need to run
<code class="highlighter-rouge">python run.py training [easy]</code>. Without specifying <code class="highlighter-rouge">easy</code>, the resulting dataset will contain
50 negative samples (<em>sampled from all domains</em>) for each true agent response. In case the flag is
specified, only 10 negative samples (<em>sampled from the same domain as the agent response</em>) will be
added for each true agent response. 
The output is stored in <code class="highlighter-rouge">stackexchange_dump/data_{allocation}.tsv</code>. A lookup <code class="highlighter-rouge">.txt</code> file is generated
for each file that contains for each row the ID of the original conversation in the source JSON.</p>



      </section>
      <footer>
        
        <p>This project was developd by <a href="https://github.com/alexanderblnf">Alex</a> during his M.Sc. under <a href="https://github.com/Guzpenha">Guzpenha</a>'s supervision</p>
        
        <p><small>Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>